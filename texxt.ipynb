{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                post          tags\n",
      "0  I  have received a message that I have won a l...  lottery scam\n",
      "1  I got an email that I won a lottery is it a scam?  lottery scam\n",
      "2   I got a  call from unknown number and have wo...  lottery scam\n",
      "3    I have deposited money to get the lottery money  lottery scam\n",
      "4                     how can I report lottery fraud  lottery scam\n",
      "5               how can I file FIR for lottery fraud  lottery scam\n",
      "6  I have transferred money to claim prize money ...  lottery scam\n",
      "7   how to make a complaint for online lottery fraud  lottery scam\n",
      "8  I have paid processing charges for lottery how...  lottery scam\n",
      "9  I have paid transfer charges for lottery how c...  lottery scam\n",
      "754\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\HP\\Downloads\\crimedatasetcsv.csv\")\n",
    "df = df[pd.notnull(df['tags'])]\n",
    "print(df.head(10))\n",
    "print(df['post'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_tags = ['lottery scam','loan fraud','job scam']\n",
    "plt.figure(figsize=(10,3))\n",
    "df.tags.value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I got a  call from unknown number and have won a prize in a lottery \n",
      "Tag: lottery scam\n"
     ]
    }
   ],
   "source": [
    "def print_plot(index):\n",
    "    example = df[df.index == index][['post', 'tags']].values[0]\n",
    "    if len(example) > 0:\n",
    "        print(example[0])\n",
    "        print('Tag:', example[1])\n",
    "print_plot(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got message saying prize\n",
      "Tag: lottery scam\n"
     ]
    }
   ],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text\n",
    "    \n",
    "df['post'] = df['post'].apply(clean_text)\n",
    "print_plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.post\n",
    "y = df.tags\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "lottery scam       0.67      0.89      0.76         9\n",
      "  loan fraud       1.00      0.17      0.29         6\n",
      "    job scam       0.82      1.00      0.90         9\n",
      "\n",
      "   micro avg       0.75      0.75      0.75        24\n",
      "   macro avg       0.83      0.69      0.65        24\n",
      "weighted avg       0.81      0.75      0.69        24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "lottery scam       0.75      1.00      0.86         9\n",
      "  loan fraud       1.00      0.67      0.80         6\n",
      "    job scam       1.00      0.89      0.94         9\n",
      "\n",
      "   micro avg       0.88      0.88      0.88        24\n",
      "   macro avg       0.92      0.85      0.87        24\n",
      "weighted avg       0.91      0.88      0.87        24\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9583333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "lottery scam       0.90      1.00      0.95         9\n",
      "  loan fraud       1.00      0.83      0.91         6\n",
      "    job scam       1.00      1.00      1.00         9\n",
      "\n",
      "   micro avg       0.96      0.96      0.96        24\n",
      "   macro avg       0.97      0.94      0.95        24\n",
      "weighted avg       0.96      0.96      0.96        24\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"f:\\GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Memorial_Hospital',\n",
       " 'Seniors',\n",
       " 'memorandum',\n",
       " 'elephant',\n",
       " 'Trump',\n",
       " 'Census',\n",
       " 'pilgrims',\n",
       " 'De',\n",
       " 'Dogs',\n",
       " '###-####_ext',\n",
       " 'chaotic',\n",
       " 'forgive',\n",
       " 'scholar',\n",
       " 'Lottery',\n",
       " 'decreasing',\n",
       " 'Supervisor',\n",
       " 'fundamentally',\n",
       " 'Fitness',\n",
       " 'abundance',\n",
       " 'Hold']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "list(islice(wv.vocab, 13030, 13050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  \n",
      "WARNING:root:cannot compute similarity with no input []\n"
     ]
    }
   ],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "    \n",
    "train, test = train_test_split(df, test_size=0.3, random_state = 42)\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values\n",
    "\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim \n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), [label]))\n",
    "    return labeled\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.post, df.tags, random_state=0, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['full-text', 'search', 'with', 'php', 'and', 'pdo', 'not', 'returning', 'any', 'result', 'i', 've', 'searched', 'a', 'lot', 'on', 'this', 'matter', 'but', 'i', 'can', 't', 'find', 'out', 'what', 's', 'wrong', 'with', 'my', 'setup.', 'i', 'm', 'trying', 'to', 'do', 'a', 'full-text', 'search', 'using', 'pdo', 'and', 'php', 'but', 'i', 'don', 't', 'get', 'any', 'results', 'or', 'error', 'messages', 'at', 'all.', 'my', 'table', 'contains', 'customer', 'details:', '<pre><code>id', 'int(11)', 'auto_increment', 'name', 'varchar(150)', 'lastname', 'varchar(150)', 'company', 'varchar(250)', 'adress', 'varchar(150)', 'postcode', 'int(5)', 'city', 'varchar(150)', 'email', 'varchar(250)', 'phone', 'varchar(20)', 'orgnr', 'varchar(15)', 'timestamp', 'timestamp', 'current_timestamp', '</code></pre>', 'i', 'did', 'run', 'the', 'sql-query:', '<pre><code>alter', 'table', 'system_customer', 'add', 'fulltext(name', 'lastname', '...', '...)', '</code></pre>', 'except', 'for', 'the', 'columns', 'id', 'postcode', 'and', 'timestamp', '.', 'no', 'signs', 'of', 'any', 'trouble', 'so', 'far.', 'i', 'have', 'no', 'idea', 'if', 'the', 'problem', 'lies', 'in', 'my', 'db', 'configuration', 'or', 'my', 'php', 'code', 'so', 'here', 'goes', 'the', 'php:', '<pre><code>$sth', '=', '$dbh-&gt;prepare(', 'select', 'name', 'lastname', 'company', 'adress', 'city', 'phone', 'email', 'orgnr', 'from', '.$db_pre.', 'customer', 'where', 'match(name', 'lastname', 'company', 'adress', 'city', 'phone', 'email', 'orgnr)', 'against(:search', 'in', 'boolean', 'mode)', ');', '//bind', 'placeholders', '$sth-&gt;bindparam(', ':search', '$data);', '$sth-&gt;execute();', '$rows', '=', '$sth-&gt;fetchall();', '//just', 'for', 'testing', 'print_r($dbh-&gt;errorinfo());', 'if(empty($rows))', '{', 'echo', '[.....]', ';', '}', 'else', '{', 'echo', '[....]', ';', 'foreach', '($rows', 'as', '$row)', '{', 'echo', '&lt;tr', 'data-href=', 'new_order.php', 'cid=', '.$row[', 'id', '].', '&gt;', ';', 'echo', '&lt;td&gt;', '.$row[', 'name', '].', '&lt;/td&gt;', ';', 'echo', '&lt;td&gt;', '.$row[', 'lastname', '].', '&lt;/td&gt;', ';', 'echo', '&lt;td&gt;', '.$row[', 'company', '].', '&lt;/td&gt;', ';', 'echo', '&lt;td&gt;', '.$row[', 'phone', '].', '&lt;/td&gt;', ';', 'echo', '&lt;td&gt;', '.$row[', 'email', '].', '&lt;/td&gt;', ';', 'echo', '&lt;td&gt;', '.date(', 'y-m-d', 'strtotime($row[', 'timestamp', '])).', '&lt;/td&gt;', ';', 'echo', '&lt;/tr&gt;', ';', '}', 'echo', '[....]', ';', '}', '</code></pre>', 'i', 'tried', 'to', 'change', 'the', 'parameter', 'in', 'the', 'searchquery', 'to', 'a', 'string', 'like', '<pre><code>against(', 'testcompany', 'somename', 'in', 'boolean', 'mode)', '</code></pre>', 'i', 'also', 'read', 'about', 'that', 'if', 'a', 'word', 'is', 'found', 'in', '50%', 'or', 'more', 'of', 'the', 'rows', 'it', 'counts', 'as', 'a', 'common', 'word.', 'i', 'm', 'pretty', 'sure', 'that', 's', 'not', 'the', 'case', 'here', '(uses', 'very', 'specific', 'words)', 'my', 'table', 'uses', 'myisam', 'engine', 'i', 'don', 't', 'get', 'any', 'results', 'or', 'any', 'error', 'messages.', 'please', 'help', 'my', 'point', 'out', 'what', 's', 'wrong', 'thank', 'you'], tags=['Train_0']),\n",
       " TaggedDocument(words=['select', 'everything', 'from', '1', 'table', 'and', 'only', 'x', 'rows', 'from', 'another', 'im', 'making', 'a', 'join', 'query', 'like:', '<pre><code>select', '*', 'from', 'clothes', 'as', 'c', 'join', 'style', 'as', 's', 'on', 'c.styleid', '=', 's.sylelid', 'where', 'clothesid', '=', '19', '</code></pre>', 'but', 'i', 'dont', 'want', 'to', 'select', 'everything', 'from', 'style', 'i', 'want', 'to', 'select', 'everything', 'from', 'clothes', '(20', 'rows)', 'and', 'only', 'select', '1', 'row', '(from', '10)', 'from', 'style', 'what', 'is', 'the', 'easyest', 'way', 'to', 'do', 'this', 'without', 'having', 'to', 'select', 'every', 'row', 'from', 'clothes', '(with', '20', 'things', 'to', 'select)', 'like:', '<pre><code>select', 'c.id', 'c.description', 'c.name', 'c.size', 'c.brand', 's.name', 'from', 'clothes', 'as', 'c', 'join', 'style', 'as', 's', 'on', 'c.styleid', '=', 'st.sylelid', 'where', 'clothesid', '=', '19', '</code></pre>', 'what', 'would', 'be', 'the', 'fastest', 'way', 'or', 'is', 'this', 'the', 'only', 'possibillity'], tags=['Train_1'])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 79232.43it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<?, ?it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 39583.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 79270.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<?, ?it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 79308.29it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 79194.55it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<?, ?it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 79270.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<?, ?it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 79156.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 79081.15it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<?, ?it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 79289.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<?, ?it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 79156.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 79213.49it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 79213.49it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 79517.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<?, ?it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 79175.63it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 79232.43it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 79137.81it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors\n",
    "    \n",
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "bad input shape (55, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-7e1774a91cf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlogreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlogreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_vectors_dbow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mlogreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_vectors_dbow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_vectors_dbow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[1;32m-> 1288\u001b[1;33m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[0;32m   1289\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    759\u001b[0m                         dtype=None)\n\u001b[0;32m    760\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'O'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bad input shape {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: bad input shape (55, 3)"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(train_vectors_dbow, y_train)\n",
    "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
    "y_pred = logreg.predict(test_vectors_dbow)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49 samples, validate on 6 samples\n",
      "Epoch 1/2\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1131 - accuracy: 0.34 - 0s 3ms/step - loss: 1.1077 - accuracy: 0.3469 - val_loss: 1.1286 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.0559 - accuracy: 0.53 - 0s 611us/step - loss: 1.0439 - accuracy: 0.6327 - val_loss: 1.1653 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "train_size = int(len(df) * .7)\n",
    "train_posts = df['post'][:train_size]\n",
    "train_tags = df['tags'][:train_size]\n",
    "\n",
    "test_posts = df['post'][train_size:]\n",
    "test_tags = df['tags'][train_size:]\n",
    "\n",
    "max_words = 1000\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_posts) # only fit on train\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 83us/step\n",
      "Test accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
